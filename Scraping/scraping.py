import requests
from bs4 import BeautifulSoup
import json
import re
import warnings
import logging
from urllib.parse import quote
from textblob import TextBlob
from textblob_fr import PatternTagger, PatternAnalyzer
import requests
import logging

HUGGINGFACE_API_KEY = "TON_API_KEY_ICI"  # Ajoute ta cl√© API ici

def optimize_query_with_ai(user_prompt):
    """
    Envoie le prompt utilisateur √† un mod√®le de langage IA pour reformuler la requ√™te en un mot-cl√© pr√©cis pour Wikip√©dia.
    """
    api_url = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct"
    headers = {"Authorization": f"Bearer {HUGGINGFACE_API_KEY}"}

    # Construire une requ√™te adapt√©e √† Wikip√©dia
    formatted_prompt = f"R√©√©cris cette question pour qu'elle corresponde exactement au titre d'une page Wikip√©dia existante dans le domaine historique : {user_prompt}"

    data = {"inputs": formatted_prompt, "parameters": {"max_length": 50}}

    try:
        response = requests.post(api_url, headers=headers, json=data)
        response.raise_for_status()
        result = response.json()
        if isinstance(result, list) and "generated_text" in result[0]:
            optimized_query = result[0]["generated_text"]
            logging.info(f"‚úÖ Requ√™te optimis√©e : {optimized_query}")
            return optimized_query
    except requests.exceptions.RequestException as e:
        logging.error(f"‚ùå Erreur API Hugging Face : {e}")
    
    return user_prompt  # Retourne le prompt original en cas d'erreur


# Configuration du logging pour le debug
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

warnings.simplefilter("ignore", ResourceWarning)

EXCLUDED_TERMS = ["film", "s√©rie t√©l√©vis√©e", "jeux vid√©o", "album", "chanson", "bande dessin√©e", "roman", "fiction"]
ROMAN_NUMERALS = ["I", "II", "III", "IV", "V", "VI", "VII", "VIII", "IX", "X", "XI", "XII", "XIII", "XIV", "XV"]

HISTORICAL_KEYWORDS = [
    "histoire", "√©v√©nement", "bataille", "r√©volution", "empire", "antiquit√©",
    "moyen √¢ge", "renaissance", "si√®cle", "guerre", "trait√©", "dynastie",
    "monarchie", "royaume", "empereur", "roi", "reine", "pr√©sident", "premier ministre",
    "constitution", "colonie", "conflit", "arm√©e", "napol√©on", "charlemagne",
    "r√©publique", "civilisation", "explorateur", "invention", "m√©morial", "aristocratie","Pr√©histoire_de_la_France",
    "Gaule",
    "Conqu√™te_de_la_Gaule",
    "Guerre_des_Gaules",
    "Vercing√©torix",
    "Province_romaine",
    "Francs",
    "Clovis_Ier",
    "Dynastie_m√©rovingienne",
    "Charles_Martel",
    "Bataille_de_Poitiers_(732)",
    "P√©pin_le_Bref",
    "Dynastie_carolingienne",
    "Charlemagne",
    "Empire_carolingien",
    "Trait√©_de_Verdun",
    "Hugues_Capet",
    "Dynastie_cap√©tienne",
    "Philippe_Auguste",
    "Guerre_de_Cent_Ans",
    "Jeanne_d'Arc",
    "Charles_VII",
    "Louis_XI",
    "Fran√ßois_Ier",
    "Renaissance_fran√ßaise",
    "Guerres_de_Religion_(France)",
    "Massacre_de_la_Saint-Barth√©lemy",
    "Henri_IV",
    "√âdit_de_Nantes",
    "Louis_XIII",
    "Cardinal_Richelieu",
    "Louis_XIV",
    "Versailles",
    "Guerre_de_Trente_Ans",
    "Colbert",
    "Louis_XV",
    "Guerre_de_Sept_Ans",
    "Louis_XVI",
    "R√©volution_fran√ßaise",
    "Prise_de_la_Bastille",
    "D√©claration_des_droits_de_l'homme_et_du_citoyen",
    "Convention_nationale",
    "Ex√©cution_de_Louis_XVI",
    "Terreur_(R√©volution_fran√ßaise)",
    "Directoire",
    "Consulat_(France)",
    "Napol√©on_Bonaparte",
    "Premier_Empire",
    "Bataille_d'Austerlitz",
    "Campagne_de_Russie",
    "Bataille_de_Waterloo",
    "Congr√®s_de_Vienne",
    "Restauration_(histoire_de_France)",
    "Charles_X",
    "R√©volution_de_Juillet",
    "Louis-Philippe_Ier",
    "Monarchie_de_Juillet",
    "R√©volution_fran√ßaise_de_1848",
    "Seconde_R√©publique_(France)",
    "Louis-Napol√©on_Bonaparte",
    "Coup_d'√âtat_du_2_d√©cembre_1851",
    "Second_Empire",
    "Guerre_de_Crim√©e",
    "Guerre_franco-allemande_de_1870",
    "Commune_de_Paris_(1871)",
    "Troisi√®me_R√©publique_(France)",
    "Affaire_Dreyfus",
    "Colonisation_fran√ßaise",
    "Premi√®re_Guerre_mondiale",
    "Bataille_de_Verdun",
    "Trait√©_de_Versailles_(1919)",
    "Entre-deux-guerres",
    "Front_populaire_(France)",
    "Seconde_Guerre_mondiale",
    "Appel_du_18_Juin",
    "R√©gime_de_Vichy",
    "Lib√©ration_de_la_France",
    "Quatri√®me_R√©publique_(France)",
    "Guerre_d'Indochine",
    "Guerre_d'Alg√©rie",
    "Charles_de_Gaulle",
    "Cinqui√®me_R√©publique",
    "Mai_68",
    "Fran√ßois_Mitterrand",
    "Trait√©_de_Maastricht",
    "Jacques_Chirac",
    "Nicolas_Sarkozy",
    "Fran√ßois_Hollande",
    "Emmanuel_Macron",
    "Union_europ√©enne",
    "Histoire_militaire_de_la_France",
    "Histoire_√©conomique_de_la_France",
    "Histoire_culturelle_de_la_France",
    "Histoire_de_la_France_contemporaine",
    "Politique_de_la_France",
    "√âconomie_de_la_France",
    "Culture_de_la_France",
    "D√©mographie_de_la_France"
]

def correct_spelling_api(text):
    """Corrige les fautes d'orthographe via LanguageTool."""
    url = "https://api.languagetool.org/v2/check"
    params = {"text": text, "language": "fr"}

    try:
        response = requests.post(url, data=params)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        logging.error(f"‚ùå Erreur API LanguageTool : {e}")
        return text

    result = response.json()
    corrected_text = text
    for match in result.get("matches", []):
        replacement = match["replacements"][0]["value"] if match["replacements"] else None
        if replacement:
            corrected_text = corrected_text[:match["offset"]] + replacement + corrected_text[match["offset"] + match["length"]:]

    return corrected_text

def extract_main_keyword(text):
    """Extrait le mot-cl√© principal en tenant compte des noms propres, chiffres romains et expressions historiques."""
    words = text.split()

    # Mots interrogatifs et inutiles √† ignorer
    question_words = {"qui", "quand", "o√π", "comment", "quel", "quelle", "quelles", "quels", "pourquoi", "est", "sont"}

    # Expressions historiques courantes √† pr√©server
    important_phrases = {"histoire de", "bataille de", "r√©volution de", "guerre de"}

    # V√©rifier si la phrase contient une expression historique connue
    for phrase in important_phrases:
        if phrase in text.lower():
            return text  # Retourne la phrase compl√®te si elle est reconnue

    # V√©rifier les noms propres et chiffres romains
    proper_nouns = []
    historical_terms = []

    i = 0
    while i < len(words):
        word = words[i].strip("?").strip(",")  # Nettoie la ponctuation

        # V√©rifier que le mot n'est pas vide avant d'acc√©der √† word[0]
        if not word:
            i += 1
            continue

        # Ignorer les mots interrogatifs
        if word.lower() in question_words:
            i += 1
            continue

        # D√©tecter les noms propres (majuscule)
        if word[0].isupper():
            temp = word

            # V√©rifie si c'est un nom propre avec chiffre romain (ex: "Henri IV")
            if i + 1 < len(words) and words[i + 1] in ROMAN_NUMERALS:
                temp += " " + words[i + 1]
                i += 1

            proper_nouns.append(temp)

        # V√©rifie si c'est un mot-cl√© historique
        elif word.lower() in HISTORICAL_KEYWORDS:
            historical_terms.append(word)

        i += 1

    # Prend un nom propre s'il y en a un
    if proper_nouns:
        return " ".join(proper_nouns)

    # Sinon, prend un mot-cl√© historique
    if historical_terms:
        return " ".join(historical_terms)

    # Si rien n'est pertinent, retourne la phrase compl√®te
    return text

def search_wikipedia(query):
    """Recherche Wikip√©dia en suivant automatiquement la redirection."""
    logging.debug(f"üîç Recherche Wikip√©dia pour : {query}")
    search_url = f"https://fr.wikipedia.org/w/index.php?search={quote(query)}"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        response = requests.get(search_url, headers=headers, timeout=5, allow_redirects=True)
        response.raise_for_status()
        
        # V√©rifie si Wikip√©dia a redirig√© directement vers une page
        if response.url.startswith("https://fr.wikipedia.org/wiki/"):
            logging.debug(f"‚úÖ Redirection suivie : {response.url}")
            return response.url

    except requests.exceptions.RequestException as e:
        logging.error(f"‚ùå Erreur Wikip√©dia : {e}")
        return None

    # Si pas de redirection directe, alors on scrute les r√©sultats
    soup = BeautifulSoup(response.text, "html.parser")
    results = soup.find_all("div", class_="mw-search-result-heading")

    for result in results:
        link = result.find("a")
        if link:
            page_url = f"https://fr.wikipedia.org{link['href']}"
            if not any(term in page_url.lower() for term in EXCLUDED_TERMS):
                return page_url

    logging.warning("‚ö† Aucune page pertinente trouv√©e.")
    return None

def get_historical_data(user_query):
    """Ex√©cute la recherche, optimise le prompt avec IA et r√©cup√®re les donn√©es Wikip√©dia."""
    original_prompt = user_query  # üîπ Sauvegarde le prompt original

    corrected_query = correct_spelling_api(user_query)  # üîπ Correction orthographique
    logging.debug(f"Correction orthographique : {user_query} ‚Üí {corrected_query}")

    optimized_query = optimize_query_with_ai(corrected_query)  # üîπ Optimisation avec IA
    logging.debug(f"Requ√™te IA optimis√©e : {optimized_query}")

    wikipedia_url = search_wikipedia(optimized_query)

    if wikipedia_url:
        # üîπ Scraping du contenu de la page Wikip√©dia
        try:
            response = requests.get(wikipedia_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            # üîπ R√©cup√®re le titre de la page
            title = soup.find("h1").text.strip()

            # üîπ R√©cup√®re tout le contenu des paragraphes
            paragraphs = soup.find_all("p")
            content = "\n\n".join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])

            wikipedia_data = {
                "prompt_utilisateur": original_prompt,
                "recherche_wikipedia": optimized_query,
                "titre": title,
                "source": wikipedia_url,
                "contenu": content
            }

            logging.info(f"‚úÖ Scraping r√©ussi pour : {title}")

        except requests.exceptions.RequestException as e:
            logging.error(f"‚ùå Erreur lors du scraping Wikip√©dia : {e}")
            wikipedia_data = {
                "prompt_utilisateur": original_prompt,
                "recherche_wikipedia": optimized_query,
                "titre": "Erreur",
                "source": wikipedia_url,
                "contenu": "Erreur de r√©cup√©ration du contenu."
            }

    else:
        wikipedia_data = {
            "prompt_utilisateur": original_prompt,
            "recherche_wikipedia": optimized_query,
            "titre": "Aucune donn√©e trouv√©e",
            "source": "Aucune source disponible",
            "contenu": "Aucune donn√©e historique disponible."
        }
        logging.warning("‚ö† Scraping √©chou√© : aucune donn√©e trouv√©e.")

    # üîπ Sauvegarde des donn√©es dans un fichier JSON
    try:
        with open("data.json", "w", encoding="utf-8") as f:
            json.dump(wikipedia_data, f, ensure_ascii=False, indent=4)
        logging.debug("üìÅ Donn√©es sauvegard√©es dans data.json")
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de l'√©criture du fichier JSON : {e}")

    return wikipedia_data

if __name__ == "__main__":
    user_query = input("Posez une question historique : ")
    result = get_historical_data(user_query)

    print(f"\nüîç Recherche pour : {result['mot_cle']}\nüîó {result['source']}\n{result['contenu'][:500]}...")

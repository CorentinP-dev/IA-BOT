import requests
from bs4 import BeautifulSoup
import json
import logging
from urllib.parse import quote

# Configuration du logging pour le debug
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# ğŸ”¹ ClÃ© API Hugging Face (Ã  renseigner)
HUGGINGFACE_API_KEY = "TON_API_KEY_ICI"

def optimize_query_with_ai(user_prompt):
    """
    Utilise une IA pour trouver directement l'URL de la page WikipÃ©dia la plus pertinente.
    """
    api_url = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct"
    headers = {"Authorization": f"Bearer {HUGGINGFACE_API_KEY}"}

    # ğŸ”¹ Demande Ã  l'IA de rechercher l'URL exacte sur WikipÃ©dia
    formatted_prompt = f"""
    Donne-moi uniquement l'URL WikipÃ©dia la plus pertinente pour cette question sans ajouter de texte supplÃ©mentaire :
    {user_prompt}

    RÃ©ponds uniquement avec l'URL complÃ¨te de la page WikipÃ©dia en franÃ§ais.
    """

    data = {"inputs": formatted_prompt, "parameters": {"max_length": 150}}

    try:
        response = requests.post(api_url, headers=headers, json=data)
        response.raise_for_status()
        result = response.json()

        if isinstance(result, list) and "generated_text" in result[0]:
            wikipedia_url = result[0]["generated_text"].strip()

            # VÃ©rifie si l'URL est valide et bien formÃ©e
            if wikipedia_url.startswith("https://fr.wikipedia.org/wiki/"):
                logging.info(f"âœ… URL trouvÃ©e par l'IA : {wikipedia_url}")
                return wikipedia_url
            else:
                logging.warning("âš  L'IA n'a pas retournÃ© une URL WikipÃ©dia valide.")
                return None
    except requests.exceptions.RequestException as e:
        logging.error(f"âŒ Erreur API Hugging Face : {e}")

    return None  # Retourne None si l'IA ne trouve pas d'URL

def search_wikipedia(query):
    """Recherche WikipÃ©dia et suit les redirections pour obtenir l'URL de l'article le plus pertinent."""
    logging.debug(f"ğŸ” Recherche WikipÃ©dia pour : {query}")
    search_url = f"https://fr.wikipedia.org/w/index.php?search={quote(query)}"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        response = requests.get(search_url, headers=headers, timeout=5, allow_redirects=True)
        response.raise_for_status()
        
        # ğŸ”¹ Si WikipÃ©dia redirige directement vers un article
        if response.url.startswith("https://fr.wikipedia.org/wiki/"):
            logging.debug(f"âœ… Redirection suivie : {response.url}")
            return response.url

    except requests.exceptions.RequestException as e:
        logging.error(f"âŒ Erreur WikipÃ©dia : {e}")
        return None

    # ğŸ”¹ Si pas de redirection, recherche dans les rÃ©sultats
    soup = BeautifulSoup(response.text, "html.parser")
    results = soup.find_all("div", class_="mw-search-result-heading")

    for result in results:
        link = result.find("a")
        if link:
            return f"https://fr.wikipedia.org{link['href']}"

    logging.warning("âš  Aucune page pertinente trouvÃ©e.")
    return None

def scrape_wikipedia(url):
    """Scrape le contenu complet d'une page WikipÃ©dia."""
    if not url:
        return None, None

    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # ğŸ”¹ RÃ©cupÃ¨re le titre de la page
        title = soup.find("h1").text.strip()

        # ğŸ”¹ RÃ©cupÃ¨re tous les paragraphes du corps de l'article
        paragraphs = soup.find_all("p")
        content = "\n\n".join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])

        return title, content

    except requests.exceptions.RequestException as e:
        logging.error(f"âŒ Erreur lors du scraping de WikipÃ©dia : {e}")
        return None, None

def get_historical_data(user_query):
    """GÃ¨re le pipeline : recherche IA sur WikipÃ©dia et scraping du contenu."""
    original_prompt = user_query  # ğŸ”¹ Sauvegarde le prompt original

    wikipedia_url = optimize_query_with_ai(user_query)  # ğŸ”¹ Recherche directe via l'IA

    if wikipedia_url:
        title, content = scrape_wikipedia(wikipedia_url)
    else:
        title, content = None, None

    wikipedia_data = {
        "prompt_utilisateur": original_prompt,
        "source": wikipedia_url if wikipedia_url else "Aucune source disponible",
        "titre": title if title else "Aucune donnÃ©e trouvÃ©e",
        "contenu": content if content else "Aucune donnÃ©e historique disponible."
    }

    # ğŸ”¹ Sauvegarde des donnÃ©es dans un fichier JSON
    try:
        with open("data.json", "w", encoding="utf-8") as f:
            json.dump(wikipedia_data, f, ensure_ascii=False, indent=4)
        logging.debug("ğŸ“ DonnÃ©es sauvegardÃ©es dans data.json")
    except Exception as e:
        logging.error(f"âŒ Erreur lors de l'Ã©criture du fichier JSON : {e}")

    return wikipedia_data

if __name__ == "__main__":
    user_query = input("Posez une question historique : ")
    result = get_historical_data(user_query)

    print(f"\nğŸ” Recherche optimisÃ©e : {result['recherche_wikipedia']}\nğŸ”— {result['source']}\nğŸ“œ {result['titre']}\n\n{result['contenu'][:500]}...")

import requests
from bs4 import BeautifulSoup
import json
import re
from spellchecker import SpellChecker

spell = SpellChecker(language="fr")  # DÃ©tecte et corrige les fautes en franÃ§ais

def correct_spelling(user_query):
    """Corrige les fautes d'orthographe dans la requÃªte utilisateur."""
    words = user_query.split()
    corrected_words = [spell.correction(word) if spell.correction(word) else word for word in words]
    return " ".join(corrected_words)


# === ğŸ” Exclure les rÃ©sultats non pertinents (films, jeux vidÃ©o, etc.) ===
EXCLUDED_TERMS = ["film", "sÃ©rie tÃ©lÃ©visÃ©e", "jeux vidÃ©o", "album", "chanson", "bande dessinÃ©e", "roman", "fiction"]

# === ğŸ” Mots-clÃ©s pour forcer la recherche historique ===
HISTORICAL_KEYWORDS = ["histoire", "Ã©vÃ©nement", "bataille", "rÃ©volution", "empire", "antiquitÃ©", "moyen Ã¢ge", 
                        "renaissance", "siÃ¨cle", "guerre", "traitÃ©", "dynastie", "monarchie", "royaume", "empereur"]

# === ğŸ“Œ Fonction pour reformuler le prompt utilisateur ===
def refine_query(user_query):
    """AmÃ©liore la requÃªte pour la rendre plus pertinente historiquement."""

    # DÃ©tection des questions et reformulation
    question_patterns = {
        r"quand (.+)": r"Date de \1",
        r"oÃ¹ (.+)": r"Lieu de \1",
        r"comment (.+)": r"Explication sur \1",
        r"qui est (.+)": r"Biographie de \1",
        r"quel est (.+)": r"DÃ©finition de \1",
        r"quelle est (.+)": r"DÃ©finition de \1",
        r"pourquoi (.+)": r"Raisons de \1",
    }

    user_query = user_query.lower().strip()

    for pattern, replacement in question_patterns.items():
        user_query = re.sub(pattern, replacement, user_query)

    if not any(word in user_query for word in HISTORICAL_KEYWORDS):
        user_query += f" {HISTORICAL_KEYWORDS[0]}"  # Ajoute "histoire" par dÃ©faut
    
    # Ajout dâ€™un mot-clÃ© historique si ce nâ€™est pas dÃ©jÃ  le cas
    if not any(word in user_query for word in HISTORICAL_KEYWORDS):
        user_query += f" {HISTORICAL_KEYWORDS[0]}"  # Ajoute "histoire" par dÃ©faut

    return user_query

# === ğŸ” Trouver l'article WikipÃ©dia le plus pertinent ===
def search_wikipedia(query):
    query = refine_query(query)  # Reformuler la requÃªte
    search_url = f"https://fr.wikipedia.org/w/index.php?search={query.replace(' ', '+')}"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        response = requests.get(search_url, headers=headers, timeout=5)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âŒ Erreur lors de la requÃªte WikipÃ©dia : {e}")
        return None  # En cas d'erreur

    soup = BeautifulSoup(response.text, "html.parser")
    results = soup.find_all("div", class_="mw-search-result-heading")

    for result in results:
        link = result.find("a")
        if link:
            relative_link = link["href"]
            page_url = f"https://fr.wikipedia.org{relative_link}"

            # VÃ©rifier que l'article ne contient pas un mot-clÃ© exclu
            if not any(term in page_url.lower() for term in EXCLUDED_TERMS):
                return page_url

    return None  # Aucun rÃ©sultat pertinent

# === ğŸ“ Scraper WikipÃ©dia avec l'article complet ===
def scrape_wikipedia(query):
    wikipedia_url = search_wikipedia(query)  # Trouver la meilleure page

    if not wikipedia_url:
        return {"titre": query, "source": "WikipÃ©dia", "contenu": "Aucune donnÃ©e historique trouvÃ©e sur WikipÃ©dia."}

    headers = {"User-Agent": "Mozilla/5.0"}
    
    try:
        response = requests.get(wikipedia_url, headers=headers, timeout=5)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âŒ Erreur lors du scraping WikipÃ©dia : {e}")
        return {"titre": query, "source": wikipedia_url, "contenu": "Erreur de rÃ©cupÃ©ration."}

    soup = BeautifulSoup(response.text, "html.parser")
    title = soup.find("h1").text

    # RÃ©cupÃ©rer tout le contenu de l'article
    content = ""
    for p in soup.find_all("p"):
        text = p.get_text().strip()
        if text:  # Ignorer les paragraphes vides
            content += text + "\n\n"

    return {"titre": title, "source": wikipedia_url, "contenu": content}


# === ğŸ“Œ Fonction principale : Recherche et Scraping ===
def get_historical_data(user_query):
    refined_query = refine_query(user_query)  # Reformuler la requÃªte
    print(f"ğŸ” Recherche pour : {refined_query}...")

    wikipedia_data = scrape_wikipedia(refined_query)

    # Sauvegarde dans un fichier JSON avec article complet
    with open("data.json", "w", encoding="utf-8") as f:
        json.dump(wikipedia_data, f, ensure_ascii=False, indent=4)

    print("âœ… Scraping terminÃ© ! L'article historique est stockÃ© dans data.json")
    return wikipedia_data


# === âœ¨ Tester avec une requÃªte utilisateur ===
if __name__ == "__main__":
    user_query = input("Posez une question historique : ")
    result = get_historical_data(user_query)

    print(f"\nğŸ“œ {result['titre']}\nğŸ”— {result['source']}\n{result['contenu'][:1000]}...")
